{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "567830e4-23e9-49f3-915c-d3af68210477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please assess the following statement and determine if it's true or false: 'Identify if the condition 'sleep apnea' is not present in the patient as mentioned in the text: 'The patient has recently been diagnosed with sleep apnea.'.' Your response should only contain 'True' or 'False'.\n"
     ]
    }
   ],
   "source": [
    "def create_prompt(entity, text, category, reasoning_style):\n",
    "    categories = {\n",
    "        \"simple\": {\n",
    "            \"positive\": f\"Determine if the condition '{entity}' is currently affecting the patient as mentioned in the text: '{text}'.\",\n",
    "            \"negated\": f\"Identify if the condition '{entity}' is not present in the patient as mentioned in the text: '{text}'.\",\n",
    "            \"possible\": f\"Assess if there is a possibility of the condition '{entity}' affecting the patient as mentioned in the text: '{text}'.\",\n",
    "            \"family\": f\"Verify if the condition '{entity}' is related to the patient's family member as mentioned in the text: '{text}'.\",\n",
    "            \"historical\": f\"Check if the condition '{entity}' is part of the patient's historical medical records as mentioned in the text: '{text}'.\",\n",
    "            \"hypothetical\": f\"Consider if the condition '{entity}' is mentioned hypothetically as it might affect the patient as mentioned in the text: '{text}'.\"\n",
    "        },\n",
    "        \"cot\": {\n",
    "            \"positive\": f\"Analyze step by step if '{entity}' is currently affecting the patient based on the sentence: '{text}'.\",\n",
    "            \"negated\": f\"Sequentially examine if '{entity}' does not exist in the patient's condition as stated in the sentence: '{text}'.\",\n",
    "            \"possible\": f\"Consider the possibility that '{entity}' could affect the patient, as suggested in the sentence: '{text}'.\",\n",
    "            \"family\": f\"Determine if '{entity}' is associated with the patient's family history as mentioned in the sentence: '{text}'.\",\n",
    "            \"historical\": f\"Investigate if '{entity}' is a part of the patient's past conditions as described in the sentence: '{text}'.\",\n",
    "            \"hypothetical\": f\"Assess if '{entity}' is discussed in a hypothetical context in the sentence: '{text}'.\"\n",
    "        },\n",
    "        \"tot\": {\n",
    "            \"positive\": f\"Explore various reasons to confirm if '{entity}' is present in the patient as indicated in the sentence: '{text}'.\",\n",
    "            \"negated\": f\"Explore various reasons to confirm if '{entity}' is absent in the patient as indicated in the sentence: '{text}'.\",\n",
    "            \"possible\": f\"Explore various possibilities that '{entity}' might be affecting the patient as indicated in the sentence: '{text}'.\",\n",
    "            \"family\": f\"Trace the family tree to see if '{entity}' is a condition shared with family members as mentioned in the sentence: '{text}'.\",\n",
    "            \"historical\": f\"Trace back through medical history to confirm if '{entity}' is a historical condition as mentioned in the sentence: '{text}'.\",\n",
    "            \"hypothetical\": f\"Consider different hypothetical scenarios where '{entity}' could potentially affect the patient as mentioned in the sentence: '{text}'.\"\n",
    "        },\n",
    "        \"self_consistency\": {\n",
    "            \"positive\": f\"Generate multiple reasoning paths to see if there is a consensus that '{entity}' is currently affecting the patient as stated in the sentence: '{text}'.\",\n",
    "            \"negated\": f\"Generate multiple reasoning paths to see if there is a consensus that '{entity}' is not currently affecting the patient as stated in the sentence: '{text}'.\",\n",
    "            \"possible\": f\"Generate multiple reasoning paths to see if there is a consensus on the possibility of '{entity}' affecting the patient as stated in the sentence: '{text}'.\",\n",
    "            \"family\": f\"Generate multiple reasoning paths to check for consistency in references to '{entity}' within the patient's family history as mentioned in the sentence: '{text}'.\",\n",
    "            \"historical\": f\"Generate multiple reasoning paths to verify if '{entity}' is consistently referred to as a historical condition in the sentence: '{text}'.\",\n",
    "            \"hypothetical\": f\"Generate multiple reasoning paths to determine if the mention of '{entity}' is consistently hypothetical as stated in the sentence: '{text}'.\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if reasoning_style.lower() not in categories or category.lower() not in categories[reasoning_style.lower()]:\n",
    "        raise ValueError(f\"The reasoning style '{reasoning_style}' with category '{category}' is not defined.\")\n",
    "    \n",
    "    reasoning_instructions = categories[reasoning_style.lower()][category.lower()]\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Please assess the following statement and determine if it's true or false: '{reasoning_instructions}'\"\n",
    "        \" Your response should only contain 'True' or 'False'.\"\n",
    "    )\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Example usage for the \"simple\" reasoning style without specific reasoning methods:\n",
    "entity = \"sleep apnea\"\n",
    "text = \"The patient has recently been diagnosed with sleep apnea.\"\n",
    "category = \"negated\"\n",
    "reasoning_style = \"simple\"\n",
    "\n",
    "# Generate the prompt\n",
    "simple_prompt = create_prompt(entity, text, category, reasoning_style)\n",
    "print(simple_prompt)\n",
    "\n",
    "# The output should be a prompt asking for a True/False response based on the category and entity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bc54883-0cce-42a4-b82b-03bc4f2b7f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "filename = \"/home/yuj49/ConText/ConText_LLM/i2b2.json\"\n",
    "import json \n",
    "# get a token: https://replicate.com/account\n",
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "REPLICATE_API_TOKEN = getpass()\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = \"r8_O1DRLBbf3PpDdwQ06vIGXnttWboYGUt2bfYE4\"\n",
    "import replicate\n",
    "# get a token: https://replicate.com/account\n",
    "from getpass import getpass\n",
    "import os\n",
    "def generate(prompt):\n",
    "    output = replicate.run(\n",
    "      \"meta/llama-2-13b-chat:f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d\",\n",
    "      input={\n",
    "        \"debug\": False,\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 1,\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": 0.75,\n",
    "        \"system_prompt\": \"You are a helpful, respectful and honest assistant. Always answer True or False.\",\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"min_new_tokens\": -1\n",
    "      }\n",
    "    )\n",
    "    res = []\n",
    "    for j in output:\n",
    "        res.append(j)  \n",
    "    # print(\" \".join(res))\n",
    "    return res\n",
    "    \n",
    "def generate_and_store_output(file, reasoning_styles):\n",
    "    new_res = []\n",
    "    print(len(file))\n",
    "    for item in file:\n",
    "        new_i = {\"label\": item[\"label\"], \"entity\": item[\"entity\"], \"file_name\": item[\"file_name\"], \"text\": item[\"text\"]}\n",
    "        for style in reasoning_styles:\n",
    "            prompt = create_prompt(item[\"entity\"], item[\"text\"], item[\"label\"], style)\n",
    "            # This is where you would interface with your language model API\n",
    "            output = generate(prompt)\n",
    "            response = \" \".join(output)\n",
    "            print(response)\n",
    "            prediction_key = f\"pred_{style}_{item['label']}\"\n",
    "            new_i[prediction_key] = response\n",
    "        new_res.append(new_i)\n",
    "    return new_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3775dda1-78cc-4a69-b811-f1333e165892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "  True .  Based  on  the  text  provided ,  the  patient  has  been  diagn osed  with  p neum onia .\n",
      "  True .  Based  on  the  given  sentence ,  there  is  a  history  of  tra uma ,  con gest ive  heart  failure ,  fe ver ,  and  p neum onia ,  which  suggests  that  the  patient  may  be  experien cing  sympt oms  related  to  p neum onia .\n",
      "  Sure !  Here ' s  my  assess ment : \n",
      " \n",
      " True\n",
      "  True .\n"
     ]
    }
   ],
   "source": [
    "filename = \"/home/yuj49/ConText/ConText_LLM/i2b2.json\"\n",
    "reasoning_styles = [\"simple\", \"cot\", \"tot\", \"self_consistency\"]\n",
    "new_res = []\n",
    "with open(filename, \"r\") as f:\n",
    "    file = json.load(f)\n",
    "    file = file[-1:]\n",
    "    print(len(file))\n",
    "    \n",
    "    for idx, item in enumerate(file):\n",
    "        new_i = {\"label\": item[\"label\"], \"entity\": item[\"entity\"], \"file_name\": item[\"file_name\"], \"text\": item[\"text\"]}\n",
    "        for style in reasoning_styles:\n",
    "            prompt = create_prompt(item[\"entity\"], item[\"text\"], item[\"label\"], style)\n",
    "            # This is where you would interface with your language model API\n",
    "            output = generate(prompt)\n",
    "            response = \" \".join(output)\n",
    "            print(response)\n",
    "            prediction_key = f\"pred_{style}_{item['label']}\"\n",
    "            new_i[prediction_key] = response\n",
    "        print(idx)\n",
    "        \n",
    "        new_res.append(new_i)\n",
    "    # results = generate_and_store_output(file, reasoning_styles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f450ff9-11e0-44db-bcc4-16e5bc567698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive',\n",
       "  'entity': 'pneumonia',\n",
       "  'file_name': 'record-54',\n",
       "  'text': 'HISTORY : Trauma , congestive heart failure , fever , pneumonia .\\n',\n",
       "  'pred_simple_positive': '  True .  Based  on  the  text  provided ,  the  patient  has  been  diagn osed  with  p neum onia .',\n",
       "  'pred_cot_positive': '  True .  Based  on  the  given  sentence ,  there  is  a  history  of  tra uma ,  con gest ive  heart  failure ,  fe ver ,  and  p neum onia ,  which  suggests  that  the  patient  may  be  experien cing  sympt oms  related  to  p neum onia .',\n",
       "  'pred_tot_positive': \"  Sure !  Here ' s  my  assess ment : \\n \\n True\",\n",
       "  'pred_self_consistency_positive': '  True .'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edb6b419-3a19-4cd6-962d-7a5d8054f92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5322\n"
     ]
    }
   ],
   "source": [
    "i2b2_llama2 = \"/home/yuj49/ConText/ConText_LLM/true_false.csv\"\n",
    "import pandas as pd\n",
    "df = pd.read_csv(i2b2_llama2)\n",
    "df_filtered = df[df['llama2'].str.contains('true|false', case=False, na=False)]\n",
    "print(len(df_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "962d8fc3-a103-4c58-903d-b2c705936415",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.to_csv(\"true_false_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9761fd9c-3a77-47cf-9464-bd16cfd0e82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5299\n"
     ]
    }
   ],
   "source": [
    "new_df = pd.read_csv(\"/home/yuj49/ConText/ConText_LLM/true_false_1.csv\")\n",
    "# 预处理数据：合并因换行而分开的行\n",
    "# 如果一行的最后有冒号':'，则将其与下一行合并\n",
    "for index in new_df.index[:-1]:  # 遍历到倒数第二行\n",
    "    if str(new_df.loc[index, 'llama2']).strip().endswith(':'):\n",
    "        new_df.loc[index, 'llama2'] += ' ' + new_df.loc[index + 1, 'llama2']\n",
    "        new_df.loc[index + 1, 'llama2'] = pd.NA  # 将下一行的内容设置为NaN\n",
    "\n",
    "# 删除被设置为NaN的行\n",
    "new_df.dropna(subset=['llama2'], inplace=True)\n",
    "\n",
    "# 筛选包含\"true\"或\"false\"（不区分大小写）的行\n",
    "df_filtered = new_df[new_df['llama2'].str.contains('true|false', case=False, na=False)]\n",
    "\n",
    "# 打印筛选后的DataFrame的长度\n",
    "print(len(df_filtered))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "313ac5bf-0342-4521-b6f4-73d378e50914",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/yuj49/ConText/ConText_LLM/llama2_i2b2.json\", \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f9943a-5c51-4d17-b976-b95b48b07e14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
